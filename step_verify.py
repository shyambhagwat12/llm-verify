import re
import torch
from enum import Enum
from typing import Annotated, Protocol, Tuple
import dspy
from dspy.predict import Retry
from dspy.functional import TypedChainOfThought
from dspy.primitives.assertions import assert_transform_module, backtrack_handler
import typer
from dsp import LM
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from dsp.modules.cache_utils import CacheMemory

# TODO: Use Chat Adapters from dspy instead of manually formatting the chat

app = typer.Typer()

DEVICE = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)


class LanguageModel(str, Enum):
    M_0_5 = "qwen:0.5b"
    M_1_8 = "qwen:latest"
    M_7_A = "meditron:7b"
    M_7_B = "mistral:v0.2"
    M_13 = "mixtral:latest"
    M_35 = "command-r:latest"
    M_70 = "meditron:70b"


class StepAnnotation(str, Enum):
    ESSENTIAL_AND_VALID = "essential_valid"
    UNNECESSARY = "unnecessary"
    LOGICALLY_FALSE = "logically_false"
    NOT_BACKED_BY_PRIOR_FACTS = "not_backed_by_prior_facts"
    BAD_DEDUCTIVE_REASONING = "bad_deductive_reasoning"
    DOES_NOT_SEEM_RIGHT = "does_not_seem_right"


class StepVerfication(dspy.Signature):
    objective: str = dspy.InputField()
    chat_history: str = dspy.InputField(
        desc="Entire Chat history with the latest message from the user as the bottom"
    )
    reasoning_chain: str = dspy.InputField(
        desc="The reasoning chain generated by the AI system in order to respond to the user's chat given their objectives"
    )
    step_to_be_verified: str = dspy.InputField()
    step_annotation: str = dspy.OutputField(
        desc=f"""Must be one of the following values: {
            [item.value for item in StepAnnotation]}"""
    )


class MessageWithUnderstanding(dspy.BaseModel):
    clear_rephrasing_of_message: str = dspy.Field(
        description="Rephrase the user's message in clearer form. Leave it empty unless a rephrasing is useful."
    )
    why_is_user_asking_this: str = dspy.Field(
        description="Why is the user asking this message at this point in the ongoing chat?"
    )
    what_is_user_objective: str = dspy.Field(
        description="What are user's overall objectives implicit or explicit within this chat?"
    )
    message_decomposition: list[str] = dspy.Field(
        description="Break down the message into simpler sub-messages."
    )


class UnderstandMessage(dspy.Signature):
    chat: list[str] = dspy.InputField(
        desc="The conversational history till now")
    new_message: str = dspy.InputField(desc="A new message by the user")
    structured_message: MessageWithUnderstanding = dspy.OutputField(
        desc="Message understood in terms of the underlying intent and objective"
    )


class ConversationalResponse(dspy.Signature):
    raw_message_from_user: str = dspy.InputField()
    structured_message: str = dspy.InputField()
    rationale: str = dspy.InputField(
        desc="Rationale behind the conversational response"
    )
    response_to_user: str = dspy.OutputField(
        desc="Response to the user in a conversational style"
    )


class Task(dspy.Module):
    def __init__(self):
        self.generate = dspy.ChainOfThought("message -> rationale, answer")

    def forward(self, structured_message: MessageWithUnderstanding) -> dspy.Prediction:
        answer = self.generate(message=str(structured_message))
        return answer


def rationale_to_steps(rationale: str) -> list[str]:
    pattern = r"(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s"
    sentences = re.split(pattern, rationale)
    # print(sentences)
    return sentences


class VerificationStrategy(str, Enum):
    LLM_AS_A_JUDGE = "llm_as_a_judge"
    RM_MODEL = "rm_model"
    BERT_CLASSIFIER = "bert_classifier"


class StepVerifierType(Protocol):
    @property
    def type(self) -> VerificationStrategy: ...

    def verify_step(
        self,
        objective: str,
        step_to_be_verified: str,
        reasoning_chain: list[str],
        chat_history: list[str] = [],
    ) -> StepAnnotation:
        """Verify this step, using this particular step verification method"""
        ...


class JudgeLmVerifier:
    def __init__(self, model: str):
        self.lm = dspy.OllamaLocal(model=model)
        self.llm_judge = TypedChainOfThought(StepVerfication)

    @property
    def type(self) -> VerificationStrategy:
        return VerificationStrategy.LLM_AS_A_JUDGE

    def verify_step(
        self,
        objective: str,
        step_to_be_verified: str,
        reasoning_chain: list[str],
        chat_history: list[str] = [],
    ) -> StepAnnotation:
        """Verify this step, using an LLM as a Judge"""
        with dspy.context(lm=self.lm):
            annotation = self.llm_judge(
                objective=objective,
                step_to_be_verified=step_to_be_verified,
                reasoning_chain="\n  - ".join(reasoning_chain),
                chat_history=chat_history,
            ).step_annotation

            print(annotation)

            return annotation


class BertClassifierVerifier:
    def __init__(self, threshold: float = 0.7, debug: bool = True):
        self.debug = debug
        self.threshold = threshold
        self.tokenizer = AutoTokenizer.from_pretrained("btan2/cappy-large")
        self.cappy = AutoModelForSequenceClassification.from_pretrained(
            "btan2/cappy-large"
        ).to(DEVICE)

    @property
    def type(self) -> VerificationStrategy:
        return VerificationStrategy.BERT_CLASSIFIER

    def verify_step(
        self,
        objective: str,
        step_to_be_verified: str,
        reasoning_chain: list[str],
        chat_history: list[str] = [],
    ) -> StepAnnotation:
        """Verify this step, using this BERT based classification models such as Cappy"""
        instruction = f"""Does the following answer meet the objective behind user's messages?

        Objectives: {objective}
        """
        instruction += "\n".join(chat_history +
                                 [f"Answer: {step_to_be_verified}"])
        response = step_to_be_verified

        print(instruction) if self.debug else ...
        print(response) if self.debug else ...

        inputs = self.tokenizer(
            [
                (instruction, response),
            ],
            return_tensors="pt",
        ).to(DEVICE)
        print(len(inputs), inputs["input_ids"].shape)
        score = self.cappy(**inputs).logits[0][0].item()

        print(score)
        return (
            StepAnnotation.DOES_NOT_SEEM_RIGHT
            if score <= self.threshold
            else StepAnnotation.ESSENTIAL_AND_VALID
        )


# class RmVerifier:
#     def __init__(self, model: str):
#         pass
#
#     @property
#     def type(self) -> VerificationStrategy: ...
#
#     def verify_step(
#         self,
#         chat_history: list[str],
#         objective: str,
#         step_to_be_verified: str,
#     ) -> StepAnnotation:
#         """Verify this step, using this particular step verification method"""
#         ...


class VerifiedQA(dspy.Module):
    def __init__(self):
        super().__init__()
        self.message_understanding = TypedChainOfThought(UnderstandMessage)
        self.task = Task()
        self.step_verifier = BertClassifierVerifier()
        self.converational = TypedChainOfThought(ConversationalResponse)
        self.objective_verifier = BertClassifierVerifier()

    def forward(
        self, message: str, chat_history: list[str] = []
    ) -> Tuple[list[str], dspy.Prediction]:
        structured_message: MessageWithUnderstanding = self.message_understanding(
            chat=chat_history, new_message=message
        ).structured_message
        # print(structured_message)

        # TODO: Format Chat and Chat history
        chat_history.append(message)

        answer = self.task(structured_message)
        steps = rationale_to_steps(answer.rationale)

        print(f"{len(steps)=}")

        chosen_steps = []

        for step in steps:
            dspy.Suggest(
                result=self.step_verifier.verify_step(
                    objective=structured_message.what_is_user_objective,
                    chat_history=chat_history,
                    reasoning_chain=steps,
                    step_to_be_verified=step,
                )
                == StepAnnotation.ESSENTIAL_AND_VALID.value,
                msg="Each step in the thought process must be necessary for reaching an answer, and be logically and factually valid.",
            )
            print("Suggest Passed!")

            chosen_steps.append(step)

        response = self.converational(
            raw_message_from_user=message,
            structured_message=str(structured_message),
            rationale=answer.answer,
        )

        objective_annotation = self.objective_verifier.verify_step(
            objective=structured_message.what_is_user_objective,
            step_to_be_verified=response.response_to_user,
            chat_history=chat_history,
            reasoning_chain=steps,
        )

        dspy.Suggest(
            result=(objective_annotation ==
                    StepAnnotation.ESSENTIAL_AND_VALID),
            msg="The answer must meet the user's objectives.",
        )

        print(objective_annotation)
        print(response)

        return chosen_steps, response


@app.command()
def chat(
    message: str,
    debug: Annotated[
        bool, typer.Option(
            help="If debug, values should be printed to stdout.")
    ] = False,
    model: Annotated[
        LanguageModel, typer.Option(help="Name of one of the local models.")
    ] = LanguageModel.M_7_B,
):
    lm = dspy.OllamaLocal(model=model.value)

    with dspy.context(lm=lm, trace=[]):
        print(CacheMemory)
        agent = assert_transform_module(
            VerifiedQA().map_named_predictors(Retry),
            backtrack_handler,
        )

        reasoning, response = agent(message)
        print(response.response_to_user)

        print(reasoning)

    if debug:
        print(lm.inspect_history(n=5))


@app.command()
def cappy(
    step: str,
    debug: Annotated[
        bool, typer.Option(
            help="If debug, values should be printed to stdout.")
    ] = False,
):
    cappy = BertClassifierVerifier()

    objective = "build a small rocket that can reach the moon"

    score = cappy.verify_step(
        objective=objective,
        step_to_be_verified=step,
        chat_history=[],
        reasoning_chain=[],
    )
    print(score)


if __name__ == "__main__":
    app()
